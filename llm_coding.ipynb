{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qyXM-l-EqmoS"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch torchaudio librosa soundfile scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/mteb/free-spoken-digit-dataset fsdd\n",
        "!ls -l fsdd/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYtnPliKt-ON",
        "outputId": "836b29df-9d9c-4783-fc5a-0c3d8baff2c8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'fsdd' already exists and is not an empty directory.\n",
            "total 19176\n",
            "-rw-r--r-- 1 root root  1980617 Sep  6 01:06 test-00000-of-00001.parquet\n",
            "-rw-r--r-- 1 root root 17651752 Sep  6 01:06 train-00000-of-00001.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_parquet('fsdd/data/train-00000-of-00001.parquet')\n",
        "df_test = pd.read_parquet('fsdd/data/test-00000-of-00001.parquet')"
      ],
      "metadata": {
        "id": "JBMrqO-s7YKc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile digits_rnn_local_fsdd.py\n",
        "\n",
        "import os, re, math, argparse, random, warnings\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import io\n",
        "import soundfile as sf\n",
        "import pandas as pd\n",
        "\n",
        "CFG = dict(\n",
        "    sr = 16000, sec = 1.0,\n",
        "    n_mfcc=40, add_deltas=True,\n",
        "    n_fft=512, hop_length=160, win_length=400, fmin=20.0, fmax=8000.0,\n",
        "    batch_size=128, epochs=30, lr=2e-4,\n",
        "    rnn_hidden=128, rnn_layers=2, rnn_bidirectional=True,\n",
        "    dropout=0.2, seed=42, patience=6\n",
        ")\n",
        "\n",
        "DIGITS = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "random.seed(CFG['seed'])\n",
        "np.random.seed(CFG['seed'])\n",
        "torch.manual_seed(CFG['seed'])\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def sec_to_frames(sec, hop, sr):\n",
        "    target_length = int(sr * sec)\n",
        "    return max(1, math.ceil((target_length - CFG[\"win_length\"]) / hop) + 1)\n",
        "\n",
        "def read_mono(audio):\n",
        "    audio_io = io.BytesIO(audio)\n",
        "    y, sr = sf.read(audio_io)\n",
        "    if y.ndim == 2: y = y.mean(axis=1)\n",
        "    return y, int(sr)\n",
        "\n",
        "class FSDDDataset(Dataset):\n",
        "    def __init__(self, df, labels, extractor, augment=False):\n",
        "      self.df = df\n",
        "      self.labels = labels\n",
        "      self.extractor = extractor\n",
        "      self.augment = augment\n",
        "    def __len__(self): return len(self.df)\n",
        "    def _aug(self, y):\n",
        "      if np.random.rand()<0.5: y = y * (10**(np.random.uniform(-6,6)/20.0))\n",
        "      if np.random.rand()<0.5:\n",
        "        s = np.random.randint(-int(0.1*len(y)), int(0.1*len(y)))\n",
        "        y = np.roll(y,s)\n",
        "      if np.random.rand()<0.3:\n",
        "        snr = np.random.choice([20,10,5]);\n",
        "        sigp = (y**2).mean();\n",
        "        y = y + np.random.randn(*y.shape)*np.sqrt(sigp/(10**(snr/10))+1e-9)\n",
        "      return y\n",
        "    def __getitem__(self, i):\n",
        "      y, sr = read_mono(self.df[i]['bytes'])\n",
        "      if self.augment: y = self._aug(y)\n",
        "      mfcc = self.extractor.extract(y, sr)\n",
        "      mfcc = self.extractor.transform(mfcc)\n",
        "      return torch.from_numpy(mfcc.T), torch.tensor(self.labels[i], dtype=torch.long)\n",
        "\n",
        "class MFCCExtractor:\n",
        "    def __init__(self, target_frames):\n",
        "      self.target_frames = target_frames\n",
        "      self.mean_, self.std_ = None, None\n",
        "    def _fix_len(self, y):\n",
        "      T = int(CFG[\"sr\"]*CFG[\"sec\"])\n",
        "      if len(y) < T:\n",
        "        y = np.pad(y, (0, T-len(y)))\n",
        "      else:\n",
        "        y = y[:T]\n",
        "      return y\n",
        "    def extract(self, y, sr):\n",
        "      if sr != CFG[\"sr\"]:\n",
        "          y = librosa.resample(y, orig_sr = sr, target_sr=CFG[\"sr\"])\n",
        "      y = self._fix_len(y)\n",
        "      mfcc = librosa.feature.mfcc(y=y, sr=CFG[\"sr\"], n_mfcc=CFG[\"n_mfcc\"], n_fft=CFG[\"n_fft\"], hop_length=CFG[\"hop_length\"], win_length=CFG[\"win_length\"],\n",
        "                                  fmin=CFG[\"fmin\"], fmax=CFG[\"fmax\"])\n",
        "      feats = [mfcc]\n",
        "      if CFG[\"add_deltas\"]:\n",
        "        feats += [librosa.feature.delta(mfcc), librosa.feature.delta(mfcc, order = 2)]\n",
        "      M = np.concatenate(feats, axis=0)\n",
        "      t = M.shape[1]\n",
        "      if t < self.target_frames:\n",
        "        M = np.pad(M, ((0,0),(0,self.target_frames - t)))\n",
        "      else:\n",
        "        M = M[:, :self.target_frames]\n",
        "      return M.astype(np.float32)\n",
        "    def fit_norm(self, feats):\n",
        "      X = np.concatenate([f.reshape(f.shape[0], -1) for f in feats], axis=1)\n",
        "      self.mean_ = X.mean(axis=1, keepdims=True)\n",
        "      self.std_ = X.std(axis=1, keepdims=True) + 1e-8\n",
        "    def transform(self, M):\n",
        "      if self.mean_ is None: return M\n",
        "      return (M - self.mean_) / self.std_\n",
        "    def save(self, path):\n",
        "      np.savez(path, mean=self.mean_, std=self.std_)\n",
        "    def load(self, path):\n",
        "      data = np.load(path)\n",
        "      self.mean_ = data['mean']\n",
        "      self.std_ = data['std']\n",
        "\n",
        "def collate_pad(batch):\n",
        "  xs, ys = zip(*batch)\n",
        "  lens = [x.shape[0] for x in xs]\n",
        "  D = xs[0].shape[1]\n",
        "  L = max(lens)\n",
        "  out = torch.zeros(len(xs), L, D, dtype=torch.float32)\n",
        "  for i,x in enumerate(xs): out[i, :x.shape[0],:] = x\n",
        "  return out, torch.stack(ys), torch.tensor(lens, dtype=torch.long)\n",
        "\n",
        "class BiLSTMClassifier(nn.Module):\n",
        "  def __init__(self, input_dim, hidden=128, num_layers=2, n_classes=10, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.LSTM(input_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=True,\n",
        "                       dropout=dropout if num_layers>1 else 0.0)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(hidden*2, n_classes)\n",
        "  def forward(self, x, lengths):\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    out, _ = self.rnn(packed)\n",
        "    out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "    mask = (torch.arange(out.size(1), device=out.device)[None,:]<lengths[:,None]).float().unsqueeze(-1)\n",
        "    out = (out*mask).sum(1) / mask.sum(1).clamp_min(1.0)\n",
        "    out = self.drop(out)\n",
        "    return self.fc(out)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for xb, yb, lengths, in loader:\n",
        "      xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
        "      ps.extend(model(xb, lengths).argmax(dim=1).cpu().tolist())\n",
        "      ys.extend(yb.cpu().tolist())\n",
        "    return accuracy_score(ys, ps), np.array(ys), np.array(ps)\n",
        "\n",
        "def train(args):\n",
        "    os.makedirs(args.out, exist_ok=True)\n",
        "    df_tr = pd.read_parquet('fsdd/data/train-00000-of-00001.parquet')\n",
        "    df_te = pd.read_parquet('fsdd/data/test-00000-of-00001.parquet')\n",
        "\n",
        "    trP = df_tr[\"audio\"]\n",
        "    trY = df_tr['label']\n",
        "    teP = df_te[\"audio\"]\n",
        "    teY = df_te['label']\n",
        "    print(f\"Split -> train: {len(trP)} | test: {len(teP)}\")\n",
        "\n",
        "    target_frames = sec_to_frames(CFG[\"sec\"], CFG[\"hop_length\"], CFG[\"sr\"])\n",
        "    extractor = MFCCExtractor(target_frames)\n",
        "    feat_list = []\n",
        "    for p in trP[:2000]:\n",
        "      y, sr = read_mono(p['bytes'])\n",
        "      feat_list.append(extractor.extract(y, sr))\n",
        "    extractor.fit_norm(feat_list)\n",
        "    extractor.save(os.path.join(args.out, \"extractor.pt\"))\n",
        "\n",
        "    full_tr = FSDDDataset(trP, trY, extractor, augment=True)\n",
        "    full_te = FSDDDataset(teP, teY, extractor, augment=False)\n",
        "    n = len(full_tr)\n",
        "    n_val = max(1, int(0.1*n))\n",
        "    n_tr = n - n_val\n",
        "    gen = torch.Generator().manual_seed(CFG[\"seed\"])\n",
        "    train_ds, val_ds = random_split(full_tr, [n_tr, n_val], generator=gen)\n",
        "\n",
        "    dl_tr = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True, collate_fn=collate_pad, num_workers=2)\n",
        "    dl_val = DataLoader(val_ds, batch_size=CFG[\"batch_size\"], shuffle=False, collate_fn=collate_pad, num_workers=2)\n",
        "    dl_te = DataLoader(full_te, batch_size=CFG[\"batch_size\"], shuffle=False, collate_fn=collate_pad, num_workers=2)\n",
        "\n",
        "    input_dim = CFG[\"n_mfcc\"]*(3 if CFG[\"add_deltas\"] else 1)\n",
        "    model = BiLSTMClassifier(input_dim, CFG[\"rnn_hidden\"], CFG[\"rnn_layers\"], 10, CFG[\"dropout\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    best_val_acc = 0.0\n",
        "    patience = CFG[\"patience\"]\n",
        "    best_state = None\n",
        "    for ep in range(1, CFG[\"epochs\"]+1):\n",
        "      model.train()\n",
        "      losses=[]\n",
        "      for xb, yb, lengths in dl_tr:\n",
        "        xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss = ce(model(xb, lengths), yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        losses.append(loss.item())\n",
        "      va_acc,_,_ = evaluate(model, dl_val, device)\n",
        "      print(f\"[E{ep:02d}] loss ={np.mean(losses):.4f} val_acc={va_acc:.4f}\")\n",
        "      if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        best_state = {k:v.detach().cpu() for k,v in model.state_dict().items()}\n",
        "        patience = CFG[\"patience\"]\n",
        "        torch.save(best_state, os.path.join(args.out, \"best_bilstm.pt\"))\n",
        "      else:\n",
        "        patience -= 1\n",
        "        if patience == 0:\n",
        "          print(f\"Early stop at epoch {ep}. Best val acc: {best_val_acc:.4f}\")\n",
        "          break\n",
        "    if best_state is not None:\n",
        "      model.load_state_dict(best_state)\n",
        "    te_acc, ys, ps = evaluate(model, dl_te, device)\n",
        "    print(f\"Test acc: {te_acc:.4f}\")\n",
        "    print(classification_report(ys, ps, target_names=DIGITS, digits=4))\n",
        "    cm = confusion_matrix(ys, ps)\n",
        "    # plot_confusion(cm, DIGITS, os.path.join(args.out, \"cm.png\"))\n",
        "    print(\"Artifacts saved ->\", args.out)\n",
        "\n",
        "\n",
        "# def inter_file(args):\n",
        "#   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#   target_frames = sec_to_frames(CFG[\"sec\"], CFG[\"hop_length\"], CFG[\"sr\"])\n",
        "#   extractor = MFCCExtractor(target_frames)\n",
        "#   extractor.load(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"extractor.pt\"))\n",
        "#   inpur_dim = CFG[\"n_mfcc\"]*(3 if CFG[\"add_deltas\"] else 1)\n",
        "#   state = torch.load(os.path.abspath)\n",
        "#   model = BiLSTMClassifier(input_dim, CFG[\"rnn_hidden\"], CFG[\"rnn_layers\"], CFG[\"rnn_bidirectional\"], CFG[\"dropout\"])\n",
        "#   model.load_state_dict(state[\"model_state_dict\"])\n",
        "#   model.to(device)\n",
        "#   model.eval()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  ap = argparse.ArgumentParser()\n",
        "  ap.add_argument(\"--mode\", choices=[\"train\", \"inter_file\"], required=True)\n",
        "  ap.add_argument(\"--data_root\", default=\"fsdd\")\n",
        "  ap.add_argument(\"--out\", default=\"results\")\n",
        "  ap.add_argument(\"--ckpt\", default=\"results/best_bilstm.pt\")\n",
        "  ap.add_argument(\"--wav\")\n",
        "  args = ap.parse_args()\n",
        "  if args.mode==\"train\":\n",
        "    train(args)\n",
        "  else:\n",
        "    inter_file(args)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWCIDBdNuN_l",
        "outputId": "a0c8028f-b282-4e8c-86bd-a449d90a5961"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting digits_rnn_local_fsdd.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python digits_rnn_local_fsdd.py --mode train --out result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIbouBbTwu0_",
        "outputId": "c867b969-1413-4ff6-f6f9-9984b1d97d32"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split -> train: 2700 | test: 300\n",
            "[E01] loss =2.2952 val_acc=0.2259\n",
            "[E02] loss =2.2697 val_acc=0.3259\n",
            "[E03] loss =2.2098 val_acc=0.3370\n",
            "[E04] loss =2.0352 val_acc=0.4889\n",
            "[E05] loss =1.7222 val_acc=0.6296\n",
            "[E06] loss =1.3763 val_acc=0.7074\n",
            "[E07] loss =1.0677 val_acc=0.7444\n",
            "[E08] loss =0.7858 val_acc=0.7593\n",
            "[E09] loss =0.5544 val_acc=0.8556\n",
            "[E10] loss =0.4516 val_acc=0.8852\n",
            "[E11] loss =0.3929 val_acc=0.9037\n",
            "[E12] loss =0.3235 val_acc=0.9111\n",
            "[E13] loss =0.3046 val_acc=0.9148\n",
            "[E14] loss =0.2794 val_acc=0.9185\n",
            "[E15] loss =0.2622 val_acc=0.9407\n",
            "[E16] loss =0.2372 val_acc=0.9074\n",
            "[E17] loss =0.2263 val_acc=0.9296\n",
            "[E18] loss =0.2001 val_acc=0.9519\n",
            "[E19] loss =0.1733 val_acc=0.9444\n",
            "[E20] loss =0.1596 val_acc=0.9259\n",
            "[E21] loss =0.1593 val_acc=0.9519\n",
            "[E22] loss =0.1491 val_acc=0.9519\n",
            "[E23] loss =0.1225 val_acc=0.9667\n",
            "[E24] loss =0.1349 val_acc=0.9593\n",
            "[E25] loss =0.1398 val_acc=0.9481\n",
            "[E26] loss =0.1285 val_acc=0.9704\n",
            "[E27] loss =0.1079 val_acc=0.9630\n",
            "[E28] loss =0.1158 val_acc=0.9481\n",
            "[E29] loss =0.1142 val_acc=0.9519\n",
            "[E30] loss =0.0919 val_acc=0.9556\n",
            "Test acc: 0.9867\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        zero     1.0000    0.9667    0.9831        30\n",
            "         one     1.0000    1.0000    1.0000        30\n",
            "         two     1.0000    1.0000    1.0000        30\n",
            "       three     0.8824    1.0000    0.9375        30\n",
            "        four     1.0000    1.0000    1.0000        30\n",
            "        five     1.0000    1.0000    1.0000        30\n",
            "         six     1.0000    0.9000    0.9474        30\n",
            "       seven     1.0000    1.0000    1.0000        30\n",
            "       eight     1.0000    1.0000    1.0000        30\n",
            "        nine     1.0000    1.0000    1.0000        30\n",
            "\n",
            "    accuracy                         0.9867       300\n",
            "   macro avg     0.9882    0.9867    0.9868       300\n",
            "weighted avg     0.9882    0.9867    0.9868       300\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/digits_rnn_local_fsdd.py\", line 240, in <module>\n",
            "    train(args)\n",
            "  File \"/content/digits_rnn_local_fsdd.py\", line 215, in train\n",
            "    plot_confusion(cm, DIGITS, os.path.join(args.out, \"cm.png\"))\n",
            "    ^^^^^^^^^^^^^^\n",
            "NameError: name 'plot_confusion' is not defined\n"
          ]
        }
      ]
    }
  ]
}